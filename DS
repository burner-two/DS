Implement Decision Tree Model using Iris dataset using Python/R and interpret
decision rules of classification.

# Import required libraries
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Decision Tree Classifier
clf = DecisionTreeClassifier(criterion='entropy', random_state=42)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Display the decision tree rules
rules = export_text(clf, feature_names=iris.feature_names)
print("\nDecision Tree Rules:\n")
print(rules)

---------------------------------------------------------------------------------------------
Load Iris Dataset. Apply K-means Algorithm using Python/R to group similar
data points into clusters. Determine optimal number of clusters using Elbow
Method. Visualize clustering results and analyze cluster characteristic


# Step 1: Import required libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Step 2: Load Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)

# Step 3: Standardize the data (important for clustering)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 4: Elbow Method to find optimal number of clusters
wcss = []  # within-cluster sum of squares
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot Elbow Graph
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o')
plt.title("Elbow Method for Optimal Clusters")
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS")
plt.grid(True)
plt.show()
 Step 5: Apply KMeans with Optimal Clusters (k=3)

# Fit KMeans with optimal number of clusters
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Add clusters to original data
X['Cluster'] = clusters

# Step 6: Visualize Clusters (using 2 principal features)
sns.pairplot(X, hue='Cluster', palette='Set1', diag_kind='kde')
plt.suptitle("Iris Clusters by K-Means", y=1.02)
plt.show()

✅ Step 7: Analyze Cluster Characteristics



# Add back original target for comparison
X['Actual Species'] = iris.target

# Analyze how clusters match actual labels
print(pd.crosstab(X['Cluster'], X['Actual Species']))

# Cluster centroids (in original feature scale)
centroids = scaler.inverse_transform(kmeans.cluster_centers_)
centroid_df = pd.DataFrame(centroids, columns=iris.feature_names)
print("\nCluster Centroids:\n")
print(centroid_df)

--------------------------------------------------------------------------------------------------

# Load Iris Dataset. Apply K-means Algorithm using Python/R to group similar
# data points into clusters. Determine optimal number of clusters using Silhouette
# analysis. Visualize clustering results and analyze cluster characteristic.

# Step 1: Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Step 2: Load the Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)

# Step 3: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 4: Silhouette Analysis to determine optimal number of clusters
silhouette_scores = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    silhouette_scores.append(score)

# Plot silhouette scores
plt.figure(figsize=(8, 5))
plt.plot(K, silhouette_scores, marker='o')
plt.title("Silhouette Score vs. Number of Clusters")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()

# Step 5: Choose best k (usually highest silhouette score)
best_k = K[silhouette_scores.index(max(silhouette_scores))]
print(f"\nOptimal number of clusters according to Silhouette Analysis: {best_k}")

# Step 6: Apply KMeans with the optimal number of clusters
kmeans = KMeans(n_clusters=best_k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Add cluster labels to original data
X['Cluster'] = clusters

# Step 7: Visualize Clustering Results
sns.pairplot(X, hue='Cluster', palette='Set2', diag_kind='kde')
plt.suptitle("K-Means Clustering on Iris Dataset (Silhouette-Based)", y=1.02)
plt.show()

# Step 8: Analyze Cluster Characteristics
# Add actual species for comparison
X['Actual Species'] = iris.target

# Compare clusters with actual species
print("\nCluster vs Actual Species:\n")
print(pd.crosstab(X['Cluster'], X['Actual Species']))

# Display cluster centroids (in original feature scale)
centroids = scaler.inverse_transform(kmeans.cluster_centers_)
centroid_df = pd.DataFrame(centroids, columns=iris.feature_names)
print("\nCluster Centroids:\n")
print(centroid_df)



____________________________________________________________________________________________________

A. Consider a scenario where you have test scores from a sample of students
and you want to compare the mean of these scores with hypothesized
population mean.
Student Score = [72, 88, 64, 74, 67, 79, 85, 75, 89,77]
Apply One Sampled T-Test using Python/R for above problem. Assume
hypothesized mean as 70. Formulate Null and Alternative Hypothesis for
a given problem. Interpret the results and draw the conclusion. [20]



# One-Sample T-Test on Student Scores
from scipy import stats

# Given student scores
student_scores = [72, 88, 64, 74, 67, 79, 85, 75, 89, 77]

# Hypothesized mean
hypothesized_mean = 70

# Apply one-sample t-test
t_statistic, p_value = stats.ttest_1samp(student_scores, hypothesized_mean)

# Display results
print(f"T-statistic: {t_statistic:.3f}")
print(f"P-value: {p_value:.4f}")

# Conclusion at 5% significance level (alpha = 0.05)
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis (H₀).")
    print("Conclusion: There is a significant difference between the sample mean and the hypothesized mean (70).")
else:
    print("Fail to reject the null hypothesis (H₀).")
    print("Conclusion: There is no significant difference between the sample mean and the hypothesized mean (70).")

_______________________________________________________________________________________________________________


Apply Feature Scaling technique like standardization and
normalization using Python/R to Boston Housing dataset. [20]



# Import libraries
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Load Boston Housing Dataset
boston = load_boston()
X = pd.DataFrame(boston.data, columns=boston.feature_names)

# Display first 5 rows of original data
print("Original Data (first 5 rows):\n")
print(X.head())

# -------------------------------
# 1. Standardization (Z-score scaling)
# Formula: z = (x - mean) / std
# -------------------------------
standard_scaler = StandardScaler()
X_standardized = standard_scaler.fit_transform(X)
X_standardized_df = pd.DataFrame(X_standardized, columns=boston.feature_names)

print("\nStandardized Data (first 5 rows):\n")
print(X_standardized_df.head())

# -------------------------------
# 2. Normalization (Min-Max Scaling)
# Formula: x' = (x - min) / (max - min)
# -------------------------------
minmax_scaler = MinMaxScaler()
X_normalized = minmax_scaler.fit_transform(X)
X_normalized_df = pd.DataFrame(X_normalized, columns=boston.feature_names)

print("\nNormalized Data (first 5 rows):\n")
print(X_normalized_df.head())



________________________________________________________________________________________________________________________


# The employee’s aptitude and job proficiency score is as follows.
# aptitude = [85, 65, 50, 68, 87, 74, 65, 96, 68, 94, 73, 84, 85, 87, 91]
# jobprof  = [70, 90, 80, 89, 88, 86, 78, 67, 86, 90, 92, 94, 99, 93, 87]
# Perform Chi-Square Test to study correlation between aptitude and job
# proficiency of employee. Formulate Null and Alternative Hypothesis for a
# given problem. Interprete results and draw conclusions based on test
# outcome.

import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency

# Input data
aptitude = [85, 65, 50, 68, 87, 74, 65, 96, 68, 94, 73, 84, 85, 87, 91]
jobprof = [70, 90, 80, 89, 88, 86, 78, 67, 86, 90, 92, 94, 99, 93, 87]

# Create a DataFrame
df = pd.DataFrame({
    'Aptitude': aptitude,
    'JobProficiency': jobprof
})

# Convert continuous variables to categorical bins
df['AptitudeGroup'] = pd.qcut(df['Aptitude'], q=3, labels=["Low", "Medium", "High"])
df['JobProfGroup'] = pd.qcut(df['JobProficiency'], q=3, labels=["Low", "Medium", "High"])

# Create contingency table
contingency_table = pd.crosstab(df['AptitudeGroup'], df['JobProfGroup'])

print("Contingency Table:\n")
print(contingency_table)

# Apply Chi-Square Test of Independence
chi2, p, dof, expected = chi2_contingency(contingency_table)

print("\nChi-Square Test Result:")
print(f"Chi-square statistic = {chi2:.4f}")
print(f"Degrees of freedom = {dof}")
print(f"P-value = {p:.4f}")

# Hypotheses
print("\nHypotheses:")
print("H₀: There is no association between Aptitude and Job Proficiency (independent)")
print("H₁: There is an association between Aptitude and Job Proficiency (dependent)")

# Conclusion
alpha = 0.05
if p < alpha:
    print("\nConclusion: Reject the null hypothesis.")
    print("There is a significant association between aptitude and job proficiency.")
else:
    print("\nConclusion: Fail to reject the null hypothesis.")
    print("There is no significant association between aptitude and job proficiency.")



_____________________________________________________________________________________________________


# Perform Logistic Regression on the Iris dataset using Python/R to predict
# binary outcome. Evaluate model’s performance using classification metrics

# Step 1: Import libraries
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Step 2: Load the Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target)

# Step 3: Convert target into binary classification problem
# Let's classify 'Setosa' (0) vs 'Non-Setosa' (1 and 2 combined)
y_binary = (y != 0).astype(int)  # Setosa = 0, Non-Setosa = 1

# Step 4: Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)

# Step 5: Train Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Step 6: Make predictions
y_pred = model.predict(X_test)

# Step 7: Evaluate model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))



__________________________________________________________________________________________________________

# Create CSV file from given data. Read the data from CSV files into a data
# frame. Perform Data pre-processing tasks such as handling missing
# values and outliers using Python/R [20]

import pandas as pd
import numpy as np

# Step 1: Sample data with missing values and outliers
data = {
    'Country': ['France', 'Spain', 'Germany', 'Spain', 'Germany', 'France', 'Spain', 'France', 'Germany', 'France'],
    'Age': [44, 27, 30, 38, np.nan, 40, 35, np.nan, 50, 100],  # 100 is an outlier
    'Salary': [72000, 48000, 54000, 61000, 58000, np.nan, 52000, 79000, np.nan, 400000],  # 400000 is an outlier
    'Purchased': ['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No']
}

# Create DataFrame
df = pd.DataFrame(data)

# Step 2: Save to CSV
csv_path = "sample_data.csv"
df.to_csv(csv_path, index=False)
print(f"CSV file created at: {csv_path}")

# Step 3: Read CSV into DataFrame
df = pd.read_csv(csv_path)
print("\nOriginal Data:\n", df)

# Step 4: Handle Missing Values
# Fill missing Age with mean
df['Age'].fillna(df['Age'].mean(), inplace=True)
# Fill missing Salary with median
df['Salary'].fillna(df['Salary'].median(), inplace=True)

# Step 5: Handle Outliers (using IQR method)
def remove_outliers_iqr(col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[col] = np.where(df[col] > upper, upper, np.where(df[col] < lower, lower, df[col]))

remove_outliers_iqr('Age')
remove_outliers_iqr('Salary')

# Step 6: Display cleaned data
print("\nCleaned Data:\n", df)



_____________________________________________________________________________________________________________


# Implement Multiple Linear Regression using Python/R on the below
# housing dataset to predict price of a house. Evaluate model’s performance
# using classification metrics. (Bedrooms , Bathrooms , Sqft_living ,floor,
# grade, Sqft_basement, Sqft_above , Price)

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split

# Step 1: Create sample housing dataset
data = {
    'Bedrooms': [3, 4, 2, 3, 5, 4, 3, 2, 6, 3],
    'Bathrooms': [1.5, 2.0, 1.0, 2.5, 3.0, 2.0, 1.0, 1.5, 4.0, 2.5],
    'Sqft_living': [1340, 1690, 900, 1600, 2200, 1850, 1200, 950, 3000, 1780],
    'Floor': [1, 2, 1, 1, 2, 1, 1, 1, 2, 2],
    'Grade': [7, 8, 6, 7, 9, 8, 6, 6, 10, 8],
    'Sqft_basement': [0, 100, 0, 200, 400, 0, 0, 100, 700, 150],
    'Sqft_above': [1340, 1590, 900, 1400, 1800, 1850, 1200, 850, 2300, 1630],
    'Price': [245000, 365000, 190000, 340000, 540000, 410000, 230000, 200000, 800000, 430000]
}

df = pd.DataFrame(data)

# Step 2: Define features and target
X = df[['Bedrooms', 'Bathrooms', 'Sqft_living', 'Floor', 'Grade', 'Sqft_basement', 'Sqft_above']]
y = df['Price']

# Step 3: Split dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# Step 4: Fit Multiple Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 5: Predict on test set
y_pred = model.predict(X_test)

# Step 6: Evaluate model
print("Mean Absolute Error (MAE):", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error (RMSE):", np.sqrt(mean_squared_error(y_test, y_pred)))
print("R-squared (R²):", r2_score(y_test, y_pred))



________________________________________________________________________________________________


# Apply Feature Scaling technique like standardization and
# normalization using Python/R to numerical features of below dataset
# (Make , Model , Color, Mileage , Sell Price)

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Step 1: Sample dataset
data = {
    'Make': ['Toyota', 'Honda', 'Ford', 'BMW', 'Nissan'],
    'Model': ['Corolla', 'Civic', 'Focus', '3 Series', 'Altima'],
    'Color': ['Red', 'Blue', 'Black', 'White', 'Silver'],
    'Mileage': [50000, 30000, 70000, 40000, 60000],
    'Sell Price': [15000, 13000, 12000, 20000, 14000]
}

df = pd.DataFrame(data)

# Step 2: Select numerical columns for scaling
num_cols = ['Mileage', 'Sell Price']
df_num = df[num_cols]

# Step 3: Apply Standardization (Z-score)
scaler_standard = StandardScaler()
standardized = scaler_standard.fit_transform(df_num)
df_standardized = pd.DataFrame(standardized, columns=[col + '_standardized' for col in num_cols])

# Step 4: Apply Normalization (Min-Max scaling)
scaler_normal = MinMaxScaler()
normalized = scaler_normal.fit_transform(df_num)
df_normalized = pd.DataFrame(normalized, columns=[col + '_normalized' for col in num_cols])

# Step 5: Combine all results
df_combined = pd.concat([df, df_standardized, df_normalized], axis=1)

# Display final dataset
print("\nScaled Dataset:\n")
print(df_combined)



___________________________________________________________________________________________


# Implement Multiple Linear Regression on the “Pima Indian Diabetes
# dataset” using Python/R

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Step 1: Load dataset from online
url = "https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv"
df = pd.read_csv(url)

# Step 2: View basic info
print("\nDataset preview:\n", df.head())

# Step 3: Define Features and Target
X = df.drop('Outcome', axis=1)        # independent variables
y = df['Outcome']                     # target (normally classification, here used as regression)

# Step 4: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 5: Fit Multiple Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 6: Predict
y_pred = model.predict(X_test)

# Step 7: Evaluate Regression Model
print("\nEvaluation Metrics:")
print("Mean Absolute Error (MAE):", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error (RMSE):", np.sqrt(mean_squared_error(y_test, y_pred)))
print("R-squared (R²):", r2_score(y_test, y_pred))


______________________________________________________________________________________________________


# Perform Logistic Regression on the Iris dataset using Python/R to predict binary outcome.
# Evaluate model’s performance using classification metrics.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.datasets import load_iris

# Step 1: Load the Iris dataset
iris = load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['Target'] = iris.target

# Step 2: Convert the problem into binary classification (Setosa vs. Non-Setosa)
df['Binary_Target'] = df['Target'].apply(lambda x: 1 if x != 0 else 0)

# Step 3: Define Features (X) and Target (y)
X = df[iris.feature_names]
y = df['Binary_Target']

# Step 4: Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 5: Fit Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Step 6: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 7: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Display results
print("Accuracy:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)


___________________________________________________________________________________________________


# Apply Feature Scaling technique like standardization and normalization
# using Python to numerical features of below dataset (Country , Age , Salary , Purchased)

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Step 1: Create sample dataset
data = {
    'Country': ['USA', 'UK', 'India', 'Canada', 'Australia'],
    'Age': [25, 30, 35, 40, 45],
    'Salary': [50000, 60000, 70000, 80000, 90000],
    'Purchased': [1, 0, 1, 0, 1]  # 1: Purchased, 0: Not Purchased
}

df = pd.DataFrame(data)

# Step 2: Select numerical columns for scaling
num_cols = ['Age', 'Salary']
df_num = df[num_cols]

# Step 3: Apply Standardization (Z-score scaling)
scaler_standard = StandardScaler()
standardized = scaler_standard.fit_transform(df_num)
df_standardized = pd.DataFrame(standardized, columns=[col + '_standardized' for col in num_cols])

# Step 4: Apply Normalization (Min-Max scaling)
scaler_normal = MinMaxScaler()
normalized = scaler_normal.fit_transform(df_num)
df_normalized = pd.DataFrame(normalized, columns=[col + '_normalized' for col in num_cols])

# Step 5: Combine all results with original dataset
df_combined = pd.concat([df, df_standardized, df_normalized], axis=1)

# Display the final dataset with scaled values
print("\nFinal Scaled Dataset:\n")
print(df_combined)
__________________________________________________________________________________________________


# Load the Iris dataset. Perform Principal Component Analysis (PCA)
# using Python to reduce dimensionality and visualize the data in the reduced-dimensional space.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Step 1: Load the Iris dataset
iris = load_iris()
X = iris.data  # Feature data (4 features)
y = iris.target  # Target labels (3 classes)

# Step 2: Standardize the data (important for PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Apply PCA
pca = PCA(n_components=2)  # Reducing to 2 components for 2D visualization
X_pca = pca.fit_transform(X_scaled)

# Step 4: Visualize the data in the reduced 2D space
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)
plt.title('PCA of Iris Dataset (2 Principal Components)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Target Class')
plt.show()

# Step 5: Explained Variance Ratio (to see how much variance is explained by each component)
print(f'Explained Variance Ratio: {pca.explained_variance_ratio_}')
print(f'Total Variance Explained by 2 Components: {sum(pca.explained_variance_ratio_):.2f}')


____-----------_____________________________________________________________________________________


# Convert Categorical Column to Numerical Representation (Feature Dummification)
# using Python to handle the 'Country' column in the dataset (Country, Age, Salary, Purchased)

import pandas as pd

# Step 1: Create sample dataset
data = {
    'Country': ['USA', 'UK', 'India', 'Canada', 'Australia'],
    'Age': [25, 30, 35, 40, 45],
    'Salary': [50000, 60000, 70000, 80000, 90000],
    'Purchased': [1, 0, 1, 0, 1]  # 1: Purchased, 0: Not Purchased
}

df = pd.DataFrame(data)

# Step 2: Apply One-Hot Encoding to the 'Country' column (Feature Dummification)
df_encoded = pd.get_dummies(df, columns=['Country'], drop_first=True)

# Step 3: Display the transformed dataset
print("\nTransformed Dataset with One-Hot Encoding:\n")
print(df_encoded)


____________________________________________________________________________________________________-

import pandas as pd

# Step 1: Create data for the cars
data = {
    'Make': ['Toyota', 'Honda', 'BMW', 'Ford', 'Chevrolet'],
    'Model': ['Corolla', 'Civic', 'X5', 'Mustang', 'Malibu'],
    'Color': ['Red', 'Blue', 'Black', 'White', 'Silver'],
    'Mileage': [30000, 25000, 15000, 20000, 18000],
    'Sell Price': [12000, 15000, 35000, 5000, 12000],
    'Buy Price': [10000, 12000, 28000, 3000, 10000]
}

# Step 2: Convert data to a DataFrame
df = pd.DataFrame(data)

# Step 3: Save the DataFrame to a CSV file
df.to_csv('car_data.csv', index=False)

# Step 4: Read the data from CSV file
df_read = pd.read_csv('car_data.csv')

# Step 5: Display records of the car having Sell Price greater than 4000
cars_above_4000 = df_read[df_read['Sell Price'] > 4000]
print("\nCars with Sell Price greater than 4000:")
print(cars_above_4000)

# Step 6: Sort the car data in ascending order based on Sell Price
sorted_cars = df_read.sort_values(by='Sell Price', ascending=True)
print("\nSorted Car Data (by Sell Price in ascending order):")
print(sorted_cars)

# Step 7: Group the data by "Make" of the car
grouped_by_make = df_read.groupby('Make').mean()  # Group by 'Make' and compute mean of numeric columns
print("\nGrouped Car Data by Make (mean values for each group):")
print(grouped_by_make)
______________________________________________________________________________________________________


# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Step 1: Load the Iris dataset
iris = load_iris()
X = iris.data  # Feature data (4 features)
y = iris.target  # Target labels (3 classes)

# Step 2: Standardize the data (important for PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Apply PCA to reduce dimensions (let's reduce to 2 principal components)
pca = PCA(n_components=2)  # Reduce to 2 components for 2D visualization
X_pca = pca.fit_transform(X_scaled)

# Step 4: Visualize the data in the reduced 2D space
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)
plt.title('PCA of Iris Dataset (2 Principal Components)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Target Class')
plt.show()

# Step 5: Explained Variance Ratio (to see how much variance is explained by each component)
print(f'Explained Variance Ratio: {pca.explained_variance_ratio_}')
print(f'Total Variance Explained by 2 Components: {sum(pca.explained_variance_ratio_):.2f}')
__________________________________________________________________________________________________________


# Import necessary libraries
import numpy as np
from scipy import stats

# Step 1: Define the exam scores for each class
class_A = [85, 90, 88, 82, 87]
class_B = [76, 78, 80, 81, 75]
class_C = [92, 88, 94, 89, 90]

# Step 2: Perform One-Way ANOVA Test
f_statistic, p_value = stats.f_oneway(class_A, class_B, class_C)

# Step 3: Display the results
print(f"F-Statistic: {f_statistic}")
print(f"P-Value: {p_value}")

# Step 4: Interpret the results
alpha = 0.05  # significance level

if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference in mean scores.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in mean scores.")


_______________________________________________________________________________________________________

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Step 1: Load the Wine Quality dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00356/winequality-red.csv"
wine_data = pd.read_csv(url, delimiter=";")

# Step 2: Inspect the dataset (optional)
print(wine_data.head())

# Step 3: Standardize the data (important for PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(wine_data.drop("quality", axis=1))  # Drop the target column 'quality'

# Step 4: Apply PCA
pca = PCA(n_components=2)  # Reduce to 2 components for 2D visualization
X_pca = pca.fit_transform(X_scaled)

# Step 5: Visualize the data in the reduced 2D space
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=wine_data['quality'], cmap='viridis', edgecolor='k', s=100)
plt.title('PCA of Wine Quality Dataset (2 Principal Components)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Wine Quality')
plt.show()

# Step 6: Explained Variance Ratio (to see how much variance is explained by each component)
print(f'Explained Variance Ratio: {pca.explained_variance_ratio_}')
print(f'Total Variance Explained by 2 Components: {sum(pca.explained_variance_ratio_):.2f}')
____________________________________________________________________________________________


# Import necessary libraries
import numpy as np
from scipy import stats

# Step 1: Define the time taken by each group
group1_time = [85, 95, 100, 80, 90, 97, 104, 95, 88, 92, 94, 99]  # Group I (0-1 year experience)
group2_time = [83, 85, 96, 92, 100, 104, 94, 95, 88, 90, 93, 94]  # Group II (1-2 years experience)

# Step 2: Perform Two-Sample T-Test
t_statistic, p_value = stats.ttest_ind(group1_time, group2_time)

# Step 3: Display the results
print(f"T-Statistic: {t_statistic}")
print(f"P-Value: {p_value}")

# Step 4: Interpret the results
alpha = 0.05  # significance level

if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference in the time taken to complete the task.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in the time taken to complete the task.")

_________________________________________________________________________________________________________________

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load the Pima Indian Diabetes dataset
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']
data = pd.read_csv(url, header=None, names=columns)

# Step 2: Inspect the dataset (optional)
print(data.head())

# Step 3: Handle missing values (optional, depending on dataset)
# In this case, we will replace zeros (which can represent missing values) with NaN and drop rows with NaN values
data.replace(0, np.nan, inplace=True)
data.dropna(inplace=True)

# Step 4: Split the data into features (X) and target (y)
X = data.drop('Outcome', axis=1)
y = data['Outcome']

# Step 5: Standardize the features (important for regression models)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 6: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 7: Create a Multiple Linear Regression model
model = LinearRegression()

# Step 8: Fit the model on the training data
model.fit(X_train, y_train)

# Step 9: Make predictions on the test data
y_pred = model.predict(X_test)

# Step 10: Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Step 11: Optionally, you can print the model coefficients to interpret the relationship between features and the outcome
print(f"Model Coefficients: {model.coef_}")
________________________________________________________________________________________________________________


# Import necessary libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Step 1: Create two separate datasets
data1 = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Wind': ['Weak', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Strong', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Strong', 'Weak'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

data2 = {
    'Outlook': ['Overcast', 'Sunny', 'Sunny', 'Rain', 'Overcast', 'Sunny', 'Rain', 'Rain', 'Overcast', 'Overcast', 'Rain', 'Sunny', 'Sunny', 'Overcast'],
    'Wind': ['Strong', 'Weak', 'Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Weak', 'Weak', 'Strong', 'Weak', 'Strong', 'Strong'],
    'PlayTennis': ['Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes']
}

# Convert the datasets into DataFrames
df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)

# Step 2: Preprocess the data (Convert categorical columns to numerical values)
def preprocess_data(df):
    df['Outlook'] = df['Outlook'].map({'Sunny': 0, 'Overcast': 1, 'Rain': 2})
    df['Wind'] = df['Wind'].map({'Weak': 0, 'Strong': 1})
    df['PlayTennis'] = df['PlayTennis'].map({'No': 0, 'Yes': 1})
    return df

df1 = preprocess_data(df1)
df2 = preprocess_data(df2)

# Step 3: Combine both datasets (if needed) or use one for training and the other for testing
df_combined = pd.concat([df1, df2], ignore_index=True)

# Step 4: Split the combined dataset into features (X) and target (y)
X = df_combined[['Outlook', 'Wind']]  # Features
y = df_combined['PlayTennis']         # Target variable

# Step 5: Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Create the Decision Tree model
model = DecisionTreeClassifier()

# Step 7: Train the model on the training data
model.fit(X_train, y_train)

# Step 8: Make predictions on the test data
y_pred = model.predict(X_test)

# Step 9: Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy * 100:.2f}%")

# Step 10: Print the predictions
print(f"Predictions: {y_pred}")
print(f"Actual values: {y_test.values}")

_____________________________________________________________________________________________________


# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.metrics import accuracy_score

# Step 1: Load the Titanic dataset
# You can load the Titanic dataset from a CSV or other sources
url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'
df = pd.read_csv(url)

# Step 2: Data Preprocessing
# Drop irrelevant columns
df = df.drop(columns=['Name', 'Ticket', 'Cabin', 'Embarked'])

# Handle missing values
df['Age'] = df['Age'].fillna(df['Age'].mean())
df['Fare'] = df['Fare'].fillna(df['Fare'].mean())

# Convert categorical column 'Sex' to numeric values
df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})

# Define the feature columns (X) and target variable (y)
X = df[['Pclass', 'Sex', 'Age', 'Fare']]
y = df['Survived']

# Step 3: Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Train the Decision Tree model
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# Step 5: Make predictions on the test data
y_pred = model.predict(X_test)

# Step 6: Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Step 7: Interpret decision rules
decision_rules = export_text(model, feature_names=['Pclass', 'Sex', 'Age', 'Fare'])
print("\nDecision Rules:\n", decision_rules)


__________________________________________________________________________________________________

import pandas as pd

# Step 1: Create the CSV file
data = {
    'make': ['Toyota', 'Honda', 'Ford', 'Chevrolet', 'BMW', 'Nissan', 'Hyundai', 'Ford', 'Chevrolet'],
    'model': ['Corolla', 'Civic', 'Focus', 'Malibu', '320i', 'Altima', 'Elantra', 'Fiesta', 'Impala'],
    'color': ['Red', 'Blue', 'Black', 'White', 'Gray', 'Silver', 'Black', 'Red', 'Blue'],
    'mileage': [30000, 25000, 22000, 18000, 15000, 20000, 25000, 30000, 27000],
    'sell_price': [8000, 9000, 7000, 7500, 15000, 9500, 8000, 8500, 10500],
    'buy_price': [2500, 3500, 4500, 5500, 13000, 5000, 4000, 3200, 3700]
}

# Convert the data to a pandas DataFrame
df = pd.DataFrame(data)

# Save the DataFrame to a CSV file
df.to_csv('car_data.csv', index=False)

# Step 2: Read the data from the CSV file
df = pd.read_csv('car_data.csv')

# Display the first few rows of the dataset
print("Car Data:")
print(df)

# Perform Transformation Functions

# 1. Display records of the car having Buy Price greater than or equal to 3000
filtered_df = df[df['buy_price'] >= 3000]
print("\nCars with Buy Price >= 3000:")
print(filtered_df)

# 2. Sort the car data in ascending order by 'buy_price'
sorted_df = df.sort_values(by='buy_price', ascending=True)
print("\nSorted Car Data by Buy Price:")
print(sorted_df)

# 3. Group the data according to the "Model" of the car and display the mean of each group
grouped_df = df.groupby('model').mean()
print("\nGrouped Data by Model (Mean Values):")
print(grouped_df)


_______________________________________________________________________________________________

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Create the dataset
data = {
    'Years_of_Experience': [2, 10, 4, 20, 8, 12, 22],
    'Salary': [30000, 95000, 45000, 178000, 84000, 120000, 200000]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Step 2: Prepare the feature matrix and target variable
X = df[['Years_of_Experience']]  # Independent variable (Years of Experience)
y = df['Salary']  # Dependent variable (Salary)

# Step 3: Split the data into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Initialize the Linear Regression model
model = LinearRegression()

# Step 5: Train the model
model.fit(X_train, y_train)

# Step 6: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 7: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Output the coefficients
print(f"Linear Regression Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared: {r2}")

# Step 8: Plot the regression line
plt.scatter(X, y, color='blue')  # Scatter plot of the original data
plt.plot(X, model.predict(X), color='red')  # Regression line
plt.title('Linear Regression: Salary vs Years of Experience')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()


__________________________________________________________________________________________________

# Import required libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the Iris dataset
from sklearn.datasets import load_iris
iris = load_iris()

# Create a DataFrame from the iris dataset
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

# Extract relevant features: petal.length and petal.width
X = df['petal length (cm)'].values.reshape(-1, 1)  # Independent variable
y = df['petal width (cm)']  # Dependent variable

# Step 1: Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Initialize the Linear Regression model
model = LinearRegression()

# Step 3: Train the model
model.fit(X_train, y_train)

# Step 4: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 5: Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the results
print(f"Linear Regression Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared: {r2}")

# Step 6: Plot the regression line
plt.scatter(X, y, color='blue')  # Scatter plot of the original data
plt.plot(X, model.predict(X), color='red')  # Regression line
plt.title('Linear Regression: Petal Width vs Petal Length')
plt.xlabel('Petal Length (cm)')
plt.ylabel('Petal Width (cm)')
plt.show()


____________________________________________________________________________________________________

# Import required libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the Iris dataset
from sklearn.datasets import load_iris
iris = load_iris()

# Create a DataFrame from the iris dataset
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

# Extract relevant features: petal.length and petal.width
X = df['petal length (cm)'].values.reshape(-1, 1)  # Independent variable
y = df['petal width (cm)']  # Dependent variable

# Step 1: Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Initialize the Linear Regression model
model = LinearRegression()

# Step 3: Train the model
model.fit(X_train, y_train)

# Step 4: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 5: Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the results
print(f"Linear Regression Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared: {r2}")

# Step 6: Plot the regression line
plt.scatter(X, y, color='blue')  # Scatter plot of the original data
plt.plot(X, model.predict(X), color='red')  # Regression line
plt.title('Linear Regression: Petal Width vs Petal Length')
plt.xlabel('Petal Length (cm)')
plt.ylabel('Petal Width (cm)')
plt.show()

_______________________________________________________________________________________________________________

# Import required libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the Iris dataset
iris = load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

# Select relevant features: petal.length and petal.width
X = df['petal length (cm)'].values.reshape(-1, 1)  # Independent variable
y = df['petal width (cm)']  # Dependent variable

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the results
print(f"Linear Regression Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared: {r2}")

# Plot the data and regression line
plt.scatter(X, y, color='blue')  # Scatter plot of the original data
plt.plot(X, model.predict(X), color='red')  # Regression line
plt.title('Linear Regression: Petal Width vs Petal Length')
plt.xlabel('Petal Length (cm)')
plt.ylabel('Petal Width (cm)')
plt.show()
______________________________________________________________________________________________________





